{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T11:38:39.419232Z",
     "start_time": "2020-11-02T11:38:39.115087Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T11:43:17.003316Z",
     "start_time": "2020-11-02T11:43:16.747480Z"
    }
   },
   "outputs": [],
   "source": [
    "# 파일 불러오기\n",
    "\n",
    "train = pd.read_csv('./open/train.csv', encoding='utf-8')\n",
    "test_x = pd.read_csv('./open/test_x.csv', encoding='utf-8')\n",
    "submission = pd.read_csv('./open/sample_submission.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T10:23:35.640983Z",
     "start_time": "2020-11-02T10:23:35.637477Z"
    }
   },
   "outputs": [],
   "source": [
    "X = train.loc[:, 'text']\n",
    "y = train.loc[:, 'author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T10:23:53.860174Z",
     "start_time": "2020-11-02T10:23:53.850111Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 1, 4, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T10:24:46.333764Z",
     "start_time": "2020-11-02T10:24:44.984940Z"
    }
   },
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(stop_words='english')\n",
    "feat_vect = count_vect.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA 객체 생성 후 Count 피처 벡터화 객체로 LDA수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T10:27:51.822143Z",
     "start_time": "2020-11-02T10:25:31.176635Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='batch', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=10,\n",
       "                          mean_change_tol=0.001, n_components=5, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=13, topic_word_prior=None,\n",
       "                          total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components=5, random_state=13)\n",
    "lda.fit(feat_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T11:25:41.995476Z",
     "start_time": "2020-11-02T11:25:41.990711Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 34416)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3.66522832, 0.22017879, 0.23746337, ..., 1.18686389, 0.200005  ,\n",
       "        0.20000261],\n",
       "       [0.24296878, 0.20964094, 0.20343087, ..., 0.21446174, 0.20857241,\n",
       "        0.20001067],\n",
       "       [0.2015402 , 0.20043959, 0.20045399, ..., 0.20186388, 0.2576835 ,\n",
       "        1.1957053 ],\n",
       "       [2.6868477 , 7.16653139, 0.2018153 , ..., 0.20000465, 0.20046145,\n",
       "        0.20058243],\n",
       "       [0.203415  , 0.20320929, 4.15683647, ..., 2.19680583, 1.13327764,\n",
       "        0.20369898]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(lda.components_.shape)\n",
    "lda.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T11:27:15.320591Z",
     "start_time": "2020-11-02T11:27:15.267870Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['000',\n",
       " '10',\n",
       " '100',\n",
       " '1000',\n",
       " '109',\n",
       " '10_s_',\n",
       " '11',\n",
       " '114th',\n",
       " '117',\n",
       " '12',\n",
       " '120',\n",
       " '126b',\n",
       " '127',\n",
       " '129',\n",
       " '12_s_',\n",
       " '12th',\n",
       " '13',\n",
       " '13th',\n",
       " '14',\n",
       " '140',\n",
       " '1429',\n",
       " '1456',\n",
       " '146m',\n",
       " '14th',\n",
       " '15',\n",
       " '150',\n",
       " '15_th',\n",
       " '15th',\n",
       " '15º',\n",
       " '16',\n",
       " '1647',\n",
       " '1676',\n",
       " '16a',\n",
       " '16th',\n",
       " '17',\n",
       " '171',\n",
       " '1715',\n",
       " '1733',\n",
       " '1742',\n",
       " '1745',\n",
       " '1748',\n",
       " '1749',\n",
       " '1750',\n",
       " '1751',\n",
       " '1756',\n",
       " '1757',\n",
       " '1764',\n",
       " '1767',\n",
       " '1772',\n",
       " '1792',\n",
       " '17__',\n",
       " '17_th_',\n",
       " '18',\n",
       " '1803',\n",
       " '1810',\n",
       " '1812',\n",
       " '1814',\n",
       " '1820',\n",
       " '1826',\n",
       " '1830',\n",
       " '1840',\n",
       " '1855',\n",
       " '1856',\n",
       " '1859',\n",
       " '1860',\n",
       " '1861',\n",
       " '1862',\n",
       " '1865',\n",
       " '1869',\n",
       " '1870',\n",
       " '1874',\n",
       " '1875',\n",
       " '1876',\n",
       " '1878',\n",
       " '1882',\n",
       " '1883',\n",
       " '1884',\n",
       " '1887',\n",
       " '1888',\n",
       " '1890',\n",
       " '1891',\n",
       " '1894',\n",
       " '1895',\n",
       " '1898',\n",
       " '18th',\n",
       " '19',\n",
       " '1908',\n",
       " '1914',\n",
       " '19o',\n",
       " '1_s_',\n",
       " '1st',\n",
       " '20',\n",
       " '200',\n",
       " '21',\n",
       " '21st',\n",
       " '22',\n",
       " '220',\n",
       " '221b',\n",
       " '22nd',\n",
       " '23',\n",
       " '23l',\n",
       " '23rd',\n",
       " '24',\n",
       " '247',\n",
       " '2473',\n",
       " '249',\n",
       " '25',\n",
       " '250',\n",
       " '25º',\n",
       " '26',\n",
       " '26th',\n",
       " '27',\n",
       " '270',\n",
       " '2704',\n",
       " '28',\n",
       " '28th',\n",
       " '29',\n",
       " '293',\n",
       " '29th',\n",
       " '2_s_',\n",
       " '2d',\n",
       " '2nd',\n",
       " '30',\n",
       " '303',\n",
       " '31',\n",
       " '32',\n",
       " '340',\n",
       " '341',\n",
       " '34th',\n",
       " '35',\n",
       " '36',\n",
       " '37',\n",
       " '3rd',\n",
       " '40',\n",
       " '4000',\n",
       " '403',\n",
       " '41',\n",
       " '421',\n",
       " '45',\n",
       " '46',\n",
       " '47',\n",
       " '470',\n",
       " '4s',\n",
       " '4th',\n",
       " '50',\n",
       " '500',\n",
       " '52',\n",
       " '534',\n",
       " '5th',\n",
       " '62o',\n",
       " '66',\n",
       " '6_d_',\n",
       " '6th',\n",
       " '700',\n",
       " '7000l',\n",
       " '750',\n",
       " '77',\n",
       " '77b',\n",
       " '7_s_',\n",
       " '7th',\n",
       " '80',\n",
       " '82',\n",
       " '84',\n",
       " '85',\n",
       " '86',\n",
       " '87',\n",
       " '88',\n",
       " '89',\n",
       " '8_d_',\n",
       " '8_s_',\n",
       " '8th',\n",
       " '93',\n",
       " '97163',\n",
       " '9th',\n",
       " '__',\n",
       " '_a',\n",
       " '_a_',\n",
       " '_absolutely_',\n",
       " '_ad',\n",
       " '_adair_',\n",
       " '_adieu',\n",
       " '_advantages_',\n",
       " '_affect_',\n",
       " '_afraid_',\n",
       " '_after',\n",
       " '_after_',\n",
       " '_afterwards',\n",
       " '_age_',\n",
       " '_ah',\n",
       " '_aimais_',\n",
       " '_air_',\n",
       " '_alea',\n",
       " '_all',\n",
       " '_all_',\n",
       " '_allow_',\n",
       " '_almost_',\n",
       " '_alone_',\n",
       " '_already_',\n",
       " '_always_',\n",
       " '_am',\n",
       " '_am_',\n",
       " '_american',\n",
       " '_amor_',\n",
       " '_amore_',\n",
       " '_and',\n",
       " '_and_',\n",
       " '_answer_',\n",
       " '_any_',\n",
       " '_appearance_',\n",
       " '_appropriation_',\n",
       " '_appulyaird',\n",
       " '_après',\n",
       " '_are',\n",
       " '_are_',\n",
       " '_arrière',\n",
       " '_art_',\n",
       " '_as',\n",
       " '_assez',\n",
       " '_at',\n",
       " '_at_',\n",
       " '_athens',\n",
       " '_au',\n",
       " '_auntie',\n",
       " '_aurora_',\n",
       " '_authority_',\n",
       " '_auto',\n",
       " '_avec',\n",
       " '_awful_',\n",
       " '_b_',\n",
       " '_bah',\n",
       " '_be',\n",
       " '_be_',\n",
       " '_bear_',\n",
       " '_beaux',\n",
       " '_because',\n",
       " '_been_',\n",
       " '_before_',\n",
       " '_begin_',\n",
       " '_being',\n",
       " '_being_',\n",
       " '_believe_',\n",
       " '_believes',\n",
       " '_bid',\n",
       " '_bid_',\n",
       " '_bienfait_',\n",
       " '_bigre_',\n",
       " '_black',\n",
       " '_blunder_',\n",
       " '_boiled_',\n",
       " '_bon_',\n",
       " '_bonjour_',\n",
       " '_bonâ',\n",
       " '_both',\n",
       " '_both_',\n",
       " '_boulanger_',\n",
       " '_bourru',\n",
       " '_brave',\n",
       " '_british',\n",
       " '_broke_',\n",
       " '_brother_',\n",
       " '_brune_',\n",
       " '_but',\n",
       " '_but_',\n",
       " '_by',\n",
       " '_c',\n",
       " '_c_',\n",
       " '_cabaret_',\n",
       " '_cafes_',\n",
       " '_called_',\n",
       " '_can',\n",
       " '_can_',\n",
       " '_candide_',\n",
       " '_cannot_',\n",
       " '_carbonari_',\n",
       " '_carte',\n",
       " '_cas',\n",
       " '_cause_',\n",
       " '_cavendish',\n",
       " '_ce',\n",
       " '_cela',\n",
       " '_certain_',\n",
       " '_certainty_',\n",
       " '_ces',\n",
       " '_cette',\n",
       " '_chaperon_',\n",
       " '_charming_',\n",
       " '_cher',\n",
       " '_cher_',\n",
       " '_chevalier_',\n",
       " '_chevaux',\n",
       " '_chez',\n",
       " '_child_',\n",
       " '_christ',\n",
       " '_christian',\n",
       " '_chronicle_',\n",
       " '_chère',\n",
       " '_chère_',\n",
       " '_cinq',\n",
       " '_come_',\n",
       " '_command',\n",
       " '_comment',\n",
       " '_commonplace_',\n",
       " '_community_',\n",
       " '_compassion_',\n",
       " '_compelled_',\n",
       " '_complete_',\n",
       " '_compliments_',\n",
       " '_compote_',\n",
       " '_con_',\n",
       " '_conditionally_',\n",
       " '_confrere_',\n",
       " '_confrère_',\n",
       " '_conqueror_',\n",
       " '_conscious_',\n",
       " '_continued_',\n",
       " '_contretemps_',\n",
       " '_corps_',\n",
       " '_could',\n",
       " '_could_',\n",
       " '_count_',\n",
       " '_coup',\n",
       " '_courtship_',\n",
       " '_credo_',\n",
       " '_crevez',\n",
       " '_cried_',\n",
       " '_curse_',\n",
       " '_cuts',\n",
       " '_d_',\n",
       " '_daily',\n",
       " '_damn_',\n",
       " '_dans',\n",
       " '_dare_',\n",
       " '_daughters_',\n",
       " '_de',\n",
       " '_dead_',\n",
       " '_deepest_',\n",
       " '_defiant',\n",
       " '_delirium_',\n",
       " '_deus',\n",
       " '_did',\n",
       " '_did_',\n",
       " '_didn',\n",
       " '_die',\n",
       " '_dieux',\n",
       " '_dined_',\n",
       " '_dines_',\n",
       " '_dio',\n",
       " '_discourteous_',\n",
       " '_disjecta',\n",
       " '_dislike_',\n",
       " '_dissolved_',\n",
       " '_distrait_',\n",
       " '_dixons_',\n",
       " '_do_',\n",
       " '_doctor',\n",
       " '_does_',\n",
       " '_don',\n",
       " '_double_',\n",
       " '_doubt_',\n",
       " '_doubts_',\n",
       " '_drap',\n",
       " '_dreams_',\n",
       " '_du',\n",
       " '_du_',\n",
       " '_débris_',\n",
       " '_dénouement_',\n",
       " '_déshabillé_',\n",
       " '_e_',\n",
       " '_each_',\n",
       " '_echo_',\n",
       " '_eclaircissement_',\n",
       " '_einen_',\n",
       " '_eldest_',\n",
       " '_eliminate_',\n",
       " '_elle',\n",
       " '_eloignez',\n",
       " '_embarras',\n",
       " '_employé_',\n",
       " '_en',\n",
       " '_encyclopædia',\n",
       " '_encyclopædia_',\n",
       " '_ends_',\n",
       " '_enfin',\n",
       " '_engaged_',\n",
       " '_engagement_',\n",
       " '_ensemble_',\n",
       " '_erect',\n",
       " '_esmeralda_',\n",
       " '_espirito',\n",
       " '_esprit_',\n",
       " '_et',\n",
       " '_etourderie_',\n",
       " '_etruria_',\n",
       " '_even',\n",
       " '_evening',\n",
       " '_evenings',\n",
       " '_everything_',\n",
       " '_excellent',\n",
       " '_exigeant_',\n",
       " '_existe',\n",
       " '_experiment_',\n",
       " '_extracted',\n",
       " '_f_',\n",
       " '_face_',\n",
       " '_facts_',\n",
       " '_fait',\n",
       " '_familiar_',\n",
       " '_farewell_',\n",
       " '_father',\n",
       " '_feigned_',\n",
       " '_felt_',\n",
       " '_few_',\n",
       " '_fey_',\n",
       " '_fiancé_',\n",
       " '_fiancée_',\n",
       " '_financial',\n",
       " '_first_',\n",
       " '_firstly_',\n",
       " '_following',\n",
       " '_folly_',\n",
       " '_food_',\n",
       " '_for',\n",
       " '_force_',\n",
       " '_forgets_',\n",
       " '_forgiven_',\n",
       " '_forgot',\n",
       " '_fortnightly',\n",
       " '_fougue',\n",
       " '_foundation_',\n",
       " '_four_',\n",
       " '_fracas_',\n",
       " '_fraternité',\n",
       " '_free_',\n",
       " '_from',\n",
       " '_fête_',\n",
       " '_g_',\n",
       " '_gamecock_',\n",
       " '_ganz_',\n",
       " '_gatzuk_',\n",
       " '_general_',\n",
       " '_geological',\n",
       " '_gigantic_',\n",
       " '_glad_',\n",
       " '_globe_',\n",
       " '_gloire',\n",
       " '_gloria',\n",
       " '_god',\n",
       " '_god_',\n",
       " '_going_',\n",
       " '_gold_',\n",
       " '_good',\n",
       " '_good_',\n",
       " '_gossip_',\n",
       " '_gott',\n",
       " '_grace',\n",
       " '_greater_',\n",
       " '_had',\n",
       " '_had_',\n",
       " '_hairry',\n",
       " '_half_',\n",
       " '_hamlet_',\n",
       " '_hand_',\n",
       " '_happy_',\n",
       " '_has_',\n",
       " '_hate_',\n",
       " '_have_',\n",
       " '_he',\n",
       " '_he_',\n",
       " '_heard_',\n",
       " '_hein',\n",
       " '_hein_',\n",
       " '_help_',\n",
       " '_her',\n",
       " '_her_',\n",
       " '_here_',\n",
       " '_hers_',\n",
       " '_herself',\n",
       " '_him',\n",
       " '_him_',\n",
       " '_him_self',\n",
       " '_his',\n",
       " '_his_',\n",
       " '_home_',\n",
       " '_hominibus',\n",
       " '_hopes_',\n",
       " '_horrible_',\n",
       " '_horror_',\n",
       " '_hotspur_',\n",
       " '_housebreaking_',\n",
       " '_how',\n",
       " '_how_',\n",
       " '_however',\n",
       " '_hundred_',\n",
       " '_hushed_',\n",
       " '_i',\n",
       " '_i_',\n",
       " '_ici_',\n",
       " '_if_',\n",
       " '_il',\n",
       " '_impressed_',\n",
       " '_in',\n",
       " '_in_',\n",
       " '_index',\n",
       " '_ingratitude_',\n",
       " '_inseparable_',\n",
       " '_insist_',\n",
       " '_interest_',\n",
       " '_interesting_',\n",
       " '_internationale',\n",
       " '_is',\n",
       " '_is_',\n",
       " '_island_',\n",
       " '_it',\n",
       " '_it_',\n",
       " '_italia',\n",
       " '_j',\n",
       " '_je',\n",
       " '_jealous_',\n",
       " '_jesus',\n",
       " '_journal',\n",
       " '_julie',\n",
       " '_july_',\n",
       " '_just_',\n",
       " '_k_',\n",
       " '_kammerherr',\n",
       " '_kammerherr_',\n",
       " '_kammerherrs_',\n",
       " '_keenly_',\n",
       " '_knew_',\n",
       " '_know_',\n",
       " '_knows_',\n",
       " '_knyght_',\n",
       " '_kratides',\n",
       " '_l',\n",
       " '_l_',\n",
       " '_la',\n",
       " '_la_',\n",
       " '_lady_',\n",
       " '_lajdak',\n",
       " '_lajdak_',\n",
       " '_lancet_',\n",
       " '_last',\n",
       " '_last_',\n",
       " '_laugh_',\n",
       " '_le',\n",
       " '_learned',\n",
       " '_lepidus',\n",
       " '_les',\n",
       " '_less_',\n",
       " '_let_',\n",
       " '_letter',\n",
       " '_letting_',\n",
       " '_lever',\n",
       " '_libertas_',\n",
       " '_like',\n",
       " '_like_',\n",
       " '_listening_',\n",
       " '_little_',\n",
       " '_lives',\n",
       " '_living_',\n",
       " '_lone',\n",
       " '_long',\n",
       " '_look_',\n",
       " '_lord_',\n",
       " '_lourdeau_',\n",
       " '_love_',\n",
       " '_m',\n",
       " '_ma',\n",
       " '_madonna_',\n",
       " '_maire',\n",
       " '_mais',\n",
       " '_manner_',\n",
       " '_many_',\n",
       " '_marchand_',\n",
       " '_maria',\n",
       " '_marlborough',\n",
       " '_marry_',\n",
       " '_master_',\n",
       " '_maternal_',\n",
       " '_matilda_',\n",
       " '_may',\n",
       " '_may_',\n",
       " '_maître_',\n",
       " '_me',\n",
       " '_me_',\n",
       " '_mean_',\n",
       " '_meaning_',\n",
       " '_meant_',\n",
       " '_mediocre_',\n",
       " '_memento',\n",
       " '_menus_',\n",
       " '_merci',\n",
       " '_merci_',\n",
       " '_mere',\n",
       " '_mes',\n",
       " '_messieurs',\n",
       " '_might_',\n",
       " '_mine',\n",
       " '_mine_',\n",
       " '_miracle_',\n",
       " '_miserable_',\n",
       " '_misery_',\n",
       " '_miss',\n",
       " '_miss_',\n",
       " '_missish_',\n",
       " '_moi',\n",
       " '_mon',\n",
       " '_monday',\n",
       " '_month_',\n",
       " '_more',\n",
       " '_more_',\n",
       " '_morning',\n",
       " '_mortal_',\n",
       " '_most_',\n",
       " '_motives_',\n",
       " '_mr',\n",
       " '_mr_',\n",
       " '_mrs',\n",
       " '_much_',\n",
       " '_must_',\n",
       " '_my',\n",
       " '_my_',\n",
       " '_my_self',\n",
       " '_myself_',\n",
       " '_mystery_',\n",
       " '_métier_',\n",
       " '_n',\n",
       " '_name_',\n",
       " '_names_',\n",
       " '_naïveté_',\n",
       " '_near_',\n",
       " '_never_',\n",
       " '_new_',\n",
       " '_no',\n",
       " '_no_',\n",
       " '_non',\n",
       " '_none_',\n",
       " '_nonesuch_',\n",
       " '_nonsense',\n",
       " '_norval_',\n",
       " '_not',\n",
       " '_not_',\n",
       " '_nothing_',\n",
       " '_notre',\n",
       " '_nous',\n",
       " '_now',\n",
       " '_now_',\n",
       " '_née_',\n",
       " '_o',\n",
       " '_o_',\n",
       " '_october_',\n",
       " '_odin',\n",
       " '_odin_',\n",
       " '_odinness_',\n",
       " '_of',\n",
       " '_of_',\n",
       " '_office_',\n",
       " '_often_',\n",
       " '_oh',\n",
       " '_omne',\n",
       " '_on',\n",
       " '_once_',\n",
       " '_one_',\n",
       " '_only_',\n",
       " '_onyegin_',\n",
       " '_or',\n",
       " '_orders_',\n",
       " '_othello_',\n",
       " '_ought_',\n",
       " '_our_',\n",
       " '_out_',\n",
       " '_outré_',\n",
       " '_ouvrier_',\n",
       " '_own_',\n",
       " '_pall',\n",
       " '_pan',\n",
       " '_pan_',\n",
       " '_panem',\n",
       " '_pani_',\n",
       " '_panie',\n",
       " '_panie_',\n",
       " '_panienotchka_',\n",
       " '_panovie',\n",
       " '_panovie_',\n",
       " '_par',\n",
       " '_parc',\n",
       " '_parce',\n",
       " '_pardon',\n",
       " '_part_',\n",
       " '_particular_',\n",
       " '_particularly_',\n",
       " '_partie',\n",
       " '_pas',\n",
       " '_passons',\n",
       " '_patriae_',\n",
       " '_pax',\n",
       " '_performance_',\n",
       " '_periodical',\n",
       " '_periodical_',\n",
       " '_peroratio_',\n",
       " '_persuasion_',\n",
       " '_petite',\n",
       " '_pitied_',\n",
       " '_pity_',\n",
       " '_plaisirs_',\n",
       " '_planning_',\n",
       " '_pledge_',\n",
       " '_port',\n",
       " '_poseurs_',\n",
       " '_possible_',\n",
       " '_post',\n",
       " '_pour',\n",
       " '_practical',\n",
       " '_present_',\n",
       " '_presume_',\n",
       " '_price_',\n",
       " '_pride_',\n",
       " '_pro',\n",
       " '_promise_',\n",
       " '_promised_',\n",
       " '_propriety_',\n",
       " '_propter',\n",
       " '_protégé_',\n",
       " '_protégés_',\n",
       " '_prove_',\n",
       " '_psychology_',\n",
       " '_pâté',\n",
       " '_qu',\n",
       " '_quantum',\n",
       " '_que',\n",
       " '_quel',\n",
       " '_quelle',\n",
       " '_quelque',\n",
       " '_quite_',\n",
       " '_radix',\n",
       " '_read',\n",
       " '_read_',\n",
       " '_real',\n",
       " '_real_',\n",
       " '_really_',\n",
       " '_rears_',\n",
       " '_reasonable_',\n",
       " '_recherché_',\n",
       " '_recollecting_',\n",
       " '_red',\n",
       " '_refuse_',\n",
       " '_remind_',\n",
       " '_repentance_',\n",
       " '_reproach',\n",
       " '_respect_',\n",
       " '_reward',\n",
       " '_rich_',\n",
       " '_right_',\n",
       " '_rigor',\n",
       " '_risus',\n",
       " '_robin_',\n",
       " '_roof_',\n",
       " '_roost_',\n",
       " '_rouge',\n",
       " '_russian_',\n",
       " '_rôle_',\n",
       " '_s',\n",
       " '_sacrifice_',\n",
       " '_said_',\n",
       " '_sainte',\n",
       " '_salon_',\n",
       " '_sans',\n",
       " '_sarah_',\n",
       " '_saw_',\n",
       " '_say',\n",
       " '_say_',\n",
       " '_scaura_',\n",
       " '_scena_',\n",
       " '_scent',\n",
       " '_scheme_',\n",
       " '_se',\n",
       " '_second_',\n",
       " '_secondly_',\n",
       " '_secret_',\n",
       " '_see_',\n",
       " '_seemed_',\n",
       " '_seems_',\n",
       " '_sensation_',\n",
       " '_sensible_',\n",
       " '_sept_',\n",
       " '_seriously_',\n",
       " '_shadow_',\n",
       " '_shall_',\n",
       " '_she',\n",
       " '_she_',\n",
       " '_ship_',\n",
       " '_shoes_',\n",
       " '_should_',\n",
       " '_shule',\n",
       " '_signor_',\n",
       " '_sine',\n",
       " '_sir',\n",
       " '_sir_',\n",
       " '_six',\n",
       " '_small_',\n",
       " '_smoke',\n",
       " '_so_',\n",
       " '_some_',\n",
       " '_something',\n",
       " '_something_',\n",
       " '_soon',\n",
       " '_sophy',\n",
       " '_sorrow',\n",
       " '_source_',\n",
       " '_st',\n",
       " '_staff_',\n",
       " '_standard_',\n",
       " '_star_',\n",
       " '_such',\n",
       " '_such_',\n",
       " '_sum',\n",
       " '_sì_',\n",
       " '_table_',\n",
       " '_tableaux',\n",
       " '_taken_',\n",
       " '_tant',\n",
       " '_telegraph_',\n",
       " '_temper_',\n",
       " '_ten_',\n",
       " '_tenez',\n",
       " '_tete',\n",
       " '_than_',\n",
       " '_that',\n",
       " '_that_',\n",
       " '_the',\n",
       " '_the_',\n",
       " '_theatre_',\n",
       " '_their_',\n",
       " '_them',\n",
       " '_them_',\n",
       " '_then_',\n",
       " '_theoretically_',\n",
       " '_there',\n",
       " '_there_',\n",
       " '_these_',\n",
       " '_they',\n",
       " '_they_',\n",
       " '_thing_',\n",
       " '_think_',\n",
       " '_third_',\n",
       " '_thirdly_',\n",
       " '_this',\n",
       " '_this_',\n",
       " '_those',\n",
       " '_thoughts_',\n",
       " '_three',\n",
       " '_through_',\n",
       " '_time_',\n",
       " '_times_',\n",
       " '_to',\n",
       " '_to_',\n",
       " '_today',\n",
       " '_today_',\n",
       " '_told_',\n",
       " '_tolerable_',\n",
       " '_tone_',\n",
       " '_too',\n",
       " '_too_',\n",
       " '_tour',\n",
       " '_tout',\n",
       " '_tout_',\n",
       " '_traditore_',\n",
       " '_trivial_',\n",
       " '_tu',\n",
       " '_two_',\n",
       " '_twofold_',\n",
       " '_tête',\n",
       " '_un',\n",
       " '_unchristian',\n",
       " '_uncle_',\n",
       " '_understand',\n",
       " '_une',\n",
       " '_universal',\n",
       " '_up_',\n",
       " '_us_',\n",
       " '_used_',\n",
       " '_v_',\n",
       " '_vater',\n",
       " '_vater_',\n",
       " '_very_',\n",
       " '_vices_',\n",
       " '_vieni_',\n",
       " '_views_',\n",
       " '_villainous_',\n",
       " '_vingt',\n",
       " '_virtuoso_',\n",
       " '_vis',\n",
       " '_vistula_',\n",
       " '_vivant_',\n",
       " '_vive',\n",
       " '_vivos',\n",
       " '_vogue',\n",
       " '_voice_',\n",
       " '_voilà',\n",
       " '_vonsohn_',\n",
       " '_voodooism',\n",
       " '_vous',\n",
       " '_voyez',\n",
       " '_wandering',\n",
       " '_want_',\n",
       " '_was',\n",
       " '_was_',\n",
       " '_wastes_',\n",
       " '_we',\n",
       " '_we_',\n",
       " '_weekly',\n",
       " '_were_',\n",
       " '_western',\n",
       " '_what',\n",
       " '_what_',\n",
       " '_when',\n",
       " '_when_',\n",
       " '_where',\n",
       " '_where_',\n",
       " '_wherein_',\n",
       " '_while',\n",
       " '_who',\n",
       " '_who_',\n",
       " '_why',\n",
       " '_why_',\n",
       " '_will_',\n",
       " '_wish_',\n",
       " '_wished_',\n",
       " '_wishes_',\n",
       " '_with',\n",
       " '_within',\n",
       " '_words_',\n",
       " '_worse',\n",
       " '_wos_',\n",
       " '_would',\n",
       " '_would_',\n",
       " '_you',\n",
       " '_you_',\n",
       " '_younger_',\n",
       " '_your',\n",
       " '_your_',\n",
       " '_yours_',\n",
       " '_yourself_',\n",
       " '_à',\n",
       " '_élite_',\n",
       " '_étape_',\n",
       " 'aaron',\n",
       " 'ab',\n",
       " 'abace',\n",
       " 'aback',\n",
       " 'abaft',\n",
       " 'abandon',\n",
       " 'abandoned',\n",
       " 'abandoning',\n",
       " 'abandons',\n",
       " 'abase',\n",
       " 'abased',\n",
       " 'abasement',\n",
       " 'abash',\n",
       " 'abashed',\n",
       " 'abasing',\n",
       " 'abated',\n",
       " 'abatement',\n",
       " 'abating',\n",
       " 'abbaye',\n",
       " 'abbess',\n",
       " 'abbey',\n",
       " 'abbeyland',\n",
       " 'abbot',\n",
       " 'abbots',\n",
       " 'abbé',\n",
       " 'abdicate',\n",
       " 'abdication',\n",
       " 'abducted',\n",
       " 'abdullah',\n",
       " 'abe',\n",
       " 'abear',\n",
       " 'abed',\n",
       " 'abel',\n",
       " 'abels',\n",
       " 'aberdeen',\n",
       " 'aberdonian',\n",
       " 'aberration',\n",
       " 'abet',\n",
       " 'abets',\n",
       " 'abetting',\n",
       " 'abeyance',\n",
       " 'abhor',\n",
       " 'abhorred',\n",
       " 'abhorrence',\n",
       " 'abhorrent',\n",
       " 'abhorring',\n",
       " 'abhors',\n",
       " 'abide',\n",
       " 'abideth',\n",
       " 'abiding',\n",
       " 'abilities',\n",
       " 'ability',\n",
       " 'abject',\n",
       " 'abjectly',\n",
       " ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T11:43:19.651071Z",
     "start_time": "2020-11-02T11:43:19.643589Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>He was almost choking. There was so much, so m...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>“Your sister asked for it, I suppose?”</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>She was engaged one day as she walked, in per...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The captain was in the porch, keeping himself ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>“Have mercy, gentlemen!” odin flung up his han...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                               text  author\n",
       "0      0  He was almost choking. There was so much, so m...       3\n",
       "1      1             “Your sister asked for it, I suppose?”       2\n",
       "2      2   She was engaged one day as she walked, in per...       1\n",
       "3      3  The captain was in the porch, keeping himself ...       4\n",
       "4      4  “Have mercy, gentlemen!” odin flung up his han...       3"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T11:44:11.201062Z",
     "start_time": "2020-11-02T11:44:11.187999Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He was almost choking. There was so much, so m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“Your sister asked for it, I suppose?”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>She was engaged one day as she walked, in per...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The captain was in the porch, keeping himself ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“Have mercy, gentlemen!” odin flung up his han...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  He was almost choking. There was so much, so m...\n",
       "1             “Your sister asked for it, I suppose?”\n",
       "2   She was engaged one day as she walked, in per...\n",
       "3  The captain was in the porch, keeping himself ...\n",
       "4  “Have mercy, gentlemen!” odin flung up his han..."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = train[['text']]\n",
    "text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word_tokenize 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T11:45:05.748212Z",
     "start_time": "2020-11-02T11:44:47.348231Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stevenkim/opt/anaconda3/envs/fc/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[He, was, almost, choking, ., There, was, so, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[“, Your, sister, asked, for, it, ,, I, suppos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[She, was, engaged, one, day, as, she, walked,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The, captain, was, in, the, porch, ,, keeping...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[“, Have, mercy, ,, gentlemen, !, ”, odin, flu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  [He, was, almost, choking, ., There, was, so, ...\n",
       "1  [“, Your, sister, asked, for, it, ,, I, suppos...\n",
       "2  [She, was, engaged, one, day, as, she, walked,...\n",
       "3  [The, captain, was, in, the, porch, ,, keeping...\n",
       "4  [“, Have, mercy, ,, gentlemen, !, ”, odin, flu..."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text['text'] = text.apply(lambda row: nltk.word_tokenize(row['text']), axis=1)\n",
    "text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 불용어 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T11:46:19.676835Z",
     "start_time": "2020-11-02T11:46:14.956325Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stevenkim/opt/anaconda3/envs/fc/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[He, almost, choking, ., There, much, ,, much,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[“, Your, sister, asked, ,, I, suppose, ?, ”]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[She, engaged, one, day, walked, ,, perusing, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The, captain, porch, ,, keeping, carefully, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[“, Have, mercy, ,, gentlemen, !, ”, odin, flu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  [He, almost, choking, ., There, much, ,, much,...\n",
       "1      [“, Your, sister, asked, ,, I, suppose, ?, ”]\n",
       "2  [She, engaged, one, day, walked, ,, perusing, ...\n",
       "3  [The, captain, porch, ,, keeping, carefully, w...\n",
       "4  [“, Have, mercy, ,, gentlemen, !, ”, odin, flu..."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "text['text'] = text['text'].apply(lambda x: [word for word in x if word not in (stop)])\n",
    "text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T12:13:18.958073Z",
     "start_time": "2020-11-02T12:13:18.955942Z"
    }
   },
   "source": [
    "### 표제어 추출로 3인칭 단수 표현을 1인칭으로 바꾸고, 과거 현재형 동사를 현재형으로 바꿈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T11:49:08.067211Z",
     "start_time": "2020-11-02T11:49:01.063965Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stevenkim/opt/anaconda3/envs/fc/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[He, almost, choke, ., There, much, ,, much, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[“, Your, sister, ask, ,, I, suppose, ?, ”]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[She, engage, one, day, walk, ,, peruse, Jane,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The, captain, porch, ,, keep, carefully, way,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[“, Have, mercy, ,, gentlemen, !, ”, odin, fli...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  [He, almost, choke, ., There, much, ,, much, w...\n",
       "1        [“, Your, sister, ask, ,, I, suppose, ?, ”]\n",
       "2  [She, engage, one, day, walk, ,, peruse, Jane,...\n",
       "3  [The, captain, porch, ,, keep, carefully, way,...\n",
       "4  [“, Have, mercy, ,, gentlemen, !, ”, odin, fli..."
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "text['text'] = text['text'].apply(lambda x: [WordNetLemmatizer().lemmatize(word, pos='v') for word in x])\n",
    "text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T11:49:35.492253Z",
     "start_time": "2020-11-02T11:49:35.489735Z"
    }
   },
   "source": [
    "### 길이가 3이하인 단어에 대해서 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T11:50:02.773783Z",
     "start_time": "2020-11-02T11:50:02.544855Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [almost, choke, There, much, much, want, stran...\n",
       "1                              [Your, sister, suppose]\n",
       "2    [engage, walk, peruse, Jane, last, letter, dwe...\n",
       "3    [captain, porch, keep, carefully, treacherous,...\n",
       "4    [Have, mercy, gentlemen, odin, fling, hand, wr...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_doc = text['text'].apply(lambda x : [word for word in x if len(word) > 3])\n",
    "tokenized_doc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF 행렬 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T11:51:10.041273Z",
     "start_time": "2020-11-02T11:51:09.459289Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stevenkim/opt/anaconda3/envs/fc/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>almost choke There much much want strange excl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Your sister suppose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>engage walk peruse Jane last letter dwell pass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>captain porch keep carefully treacherous shoot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Have mercy gentlemen odin fling hand write any...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  almost choke There much much want strange excl...\n",
       "1                                Your sister suppose\n",
       "2  engage walk peruse Jane last letter dwell pass...\n",
       "3  captain porch keep carefully treacherous shoot...\n",
       "4  Have mercy gentlemen odin fling hand write any..."
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 역토큰화\n",
    "detokenized_doc = []\n",
    "for i in range(len(text)):\n",
    "    t = ' '.join(tokenized_doc[i])\n",
    "    detokenized_doc.append(t)\n",
    "text['text'] = detokenized_doc\n",
    "# 다시 text['text'] 에 저장\n",
    "text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorizer를 통해 단어 1,000개에 대한 TF-IDF 행렬 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T12:15:42.533589Z",
     "start_time": "2020-11-02T12:15:41.698573Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "X = vectorizer.fit_transform(text['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T12:15:42.538673Z",
     "start_time": "2020-11-02T12:15:42.535274Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54879, 1000)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토픽 모델링(LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T12:15:58.587045Z",
     "start_time": "2020-11-02T12:15:43.560863Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda_model = LatentDirichletAllocation(n_components=5, learning_method='online', random_state=13, max_iter=1)\n",
    "lda_top = lda_model.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T12:15:58.592103Z",
     "start_time": "2020-11-02T12:15:58.588920Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.20375636   0.20392265   0.20428166 ...   1.14980745 170.83048358\n",
      "    0.20754686]\n",
      " [118.84436233  40.6666545    0.20861991 ... 128.76185146   0.20951396\n",
      "    0.21306149]\n",
      " [  0.2041357    0.20410066  77.53765712 ...   1.7497157   92.49617708\n",
      "    0.20769866]\n",
      " [  7.46251627   0.20222935   0.20548765 ...   0.20296145 118.04304008\n",
      "   47.27492699]\n",
      " [  0.2918131    0.2042045    0.20445965 ...   0.23437028  51.78250206\n",
      "    0.20397706]]\n"
     ]
    }
   ],
   "source": [
    "print(lda_model.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T12:15:58.597873Z",
     "start_time": "2020-11-02T12:15:58.594648Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1000)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.components_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어 집합, 1,000개의 단어가 저장되어있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T12:15:58.602004Z",
     "start_time": "2020-11-02T12:15:58.599429Z"
    }
   },
   "outputs": [],
   "source": [
    "terms = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T12:16:40.109342Z",
     "start_time": "2020-11-02T12:16:40.092288Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,\n",
       " ['able',\n",
       "  'abroad',\n",
       "  'absolutely',\n",
       "  'accept',\n",
       "  'accompany',\n",
       "  'account',\n",
       "  'acquaintance',\n",
       "  'action',\n",
       "  'actually',\n",
       "  'address',\n",
       "  'admire',\n",
       "  'admit',\n",
       "  'advance',\n",
       "  'advantage',\n",
       "  'adventure',\n",
       "  'advice',\n",
       "  'affair',\n",
       "  'affairs',\n",
       "  'affect',\n",
       "  'affection',\n",
       "  'afraid',\n",
       "  'afternoon',\n",
       "  'agree',\n",
       "  'agreeable',\n",
       "  'alarm',\n",
       "  'alive',\n",
       "  'allow',\n",
       "  'aloud',\n",
       "  'altogether',\n",
       "  'amazement',\n",
       "  'amuse',\n",
       "  'anger',\n",
       "  'angry',\n",
       "  'anne',\n",
       "  'announce',\n",
       "  'answer',\n",
       "  'anxiety',\n",
       "  'anxious',\n",
       "  'anybody',\n",
       "  'apparently',\n",
       "  'appear',\n",
       "  'appearance',\n",
       "  'approach',\n",
       "  'arrange',\n",
       "  'arrest',\n",
       "  'arrival',\n",
       "  'arrive',\n",
       "  'article',\n",
       "  'ashamed',\n",
       "  'aside',\n",
       "  'asleep',\n",
       "  'assure',\n",
       "  'astonishment',\n",
       "  'attack',\n",
       "  'attempt',\n",
       "  'attend',\n",
       "  'attention',\n",
       "  'aunt',\n",
       "  'avoid',\n",
       "  'aware',\n",
       "  'away',\n",
       "  'ball',\n",
       "  'bank',\n",
       "  'bath',\n",
       "  'bear',\n",
       "  'beat',\n",
       "  'beautiful',\n",
       "  'beauty',\n",
       "  'begin',\n",
       "  'behaviour',\n",
       "  'behold',\n",
       "  'believe',\n",
       "  'bell',\n",
       "  'belong',\n",
       "  'bend',\n",
       "  'best',\n",
       "  'better',\n",
       "  'bind',\n",
       "  'bird',\n",
       "  'bite',\n",
       "  'black',\n",
       "  'blame',\n",
       "  'bless',\n",
       "  'blind',\n",
       "  'blood',\n",
       "  'blow',\n",
       "  'blue',\n",
       "  'blush',\n",
       "  'board',\n",
       "  'boat',\n",
       "  'bodinm',\n",
       "  'body',\n",
       "  'book',\n",
       "  'boot',\n",
       "  'bottle',\n",
       "  'boys',\n",
       "  'brain',\n",
       "  'break',\n",
       "  'breakfast',\n",
       "  'breast',\n",
       "  'breath',\n",
       "  'breathe',\n",
       "  'bright',\n",
       "  'bring',\n",
       "  'broad',\n",
       "  'brother',\n",
       "  'brown',\n",
       "  'build',\n",
       "  'burn',\n",
       "  'burst',\n",
       "  'business',\n",
       "  'busy',\n",
       "  'bye',\n",
       "  'candle',\n",
       "  'capable',\n",
       "  'captain',\n",
       "  'card',\n",
       "  'care',\n",
       "  'carefully',\n",
       "  'carriage',\n",
       "  'carry',\n",
       "  'case',\n",
       "  'cast',\n",
       "  'catch',\n",
       "  'cause',\n",
       "  'cease',\n",
       "  'certain',\n",
       "  'certainly',\n",
       "  'chair',\n",
       "  'chamber',\n",
       "  'chance',\n",
       "  'change',\n",
       "  'character',\n",
       "  'charge',\n",
       "  'charm',\n",
       "  'cheek',\n",
       "  'chief',\n",
       "  'child',\n",
       "  'children',\n",
       "  'choose',\n",
       "  'church',\n",
       "  'circle',\n",
       "  'circumstance',\n",
       "  'circumstances',\n",
       "  'city',\n",
       "  'claim',\n",
       "  'class',\n",
       "  'clean',\n",
       "  'clear',\n",
       "  'clearly',\n",
       "  'clerk',\n",
       "  'clever',\n",
       "  'clock',\n",
       "  'close',\n",
       "  'clothe',\n",
       "  'cloud',\n",
       "  'coach',\n",
       "  'coat',\n",
       "  'cold',\n",
       "  'colour',\n",
       "  'come',\n",
       "  'comfort',\n",
       "  'comfortable',\n",
       "  'command',\n",
       "  'commit',\n",
       "  'common',\n",
       "  'companion',\n",
       "  'company',\n",
       "  'complete',\n",
       "  'completely',\n",
       "  'compliment',\n",
       "  'conceal',\n",
       "  'concern',\n",
       "  'conclude',\n",
       "  'conclusion',\n",
       "  'condition',\n",
       "  'conduct',\n",
       "  'confess',\n",
       "  'confidence',\n",
       "  'confusion',\n",
       "  'conscience',\n",
       "  'conscious',\n",
       "  'consent',\n",
       "  'consequence',\n",
       "  'consider',\n",
       "  'considerable',\n",
       "  'consideration',\n",
       "  'content',\n",
       "  'continue',\n",
       "  'contrary',\n",
       "  'conversation',\n",
       "  'conviction',\n",
       "  'convince',\n",
       "  'corner',\n",
       "  'count',\n",
       "  'countenance',\n",
       "  'country',\n",
       "  'couple',\n",
       "  'course',\n",
       "  'court',\n",
       "  'cousin',\n",
       "  'cover',\n",
       "  'creature',\n",
       "  'crime',\n",
       "  'cross',\n",
       "  'crowd',\n",
       "  'curiosity',\n",
       "  'curious',\n",
       "  'curse',\n",
       "  'dance',\n",
       "  'danger',\n",
       "  'dangerous',\n",
       "  'dare',\n",
       "  'dark',\n",
       "  'darkness',\n",
       "  'daughter',\n",
       "  'day',\n",
       "  'days',\n",
       "  'dead',\n",
       "  'deal',\n",
       "  'dear',\n",
       "  'death',\n",
       "  'deceive',\n",
       "  'decide',\n",
       "  'declare',\n",
       "  'deep',\n",
       "  'degree',\n",
       "  'delight',\n",
       "  'demand',\n",
       "  'deny',\n",
       "  'depend',\n",
       "  'desert',\n",
       "  'deserve',\n",
       "  'desire',\n",
       "  'despair',\n",
       "  'determine',\n",
       "  'devil',\n",
       "  'difference',\n",
       "  'different',\n",
       "  'difficult',\n",
       "  'difficulty',\n",
       "  'dignity',\n",
       "  'dinner',\n",
       "  'direct',\n",
       "  'direction',\n",
       "  'directly',\n",
       "  'disappear',\n",
       "  'discover',\n",
       "  'disgrace',\n",
       "  'dispose',\n",
       "  'distance',\n",
       "  'distress',\n",
       "  'disturb',\n",
       "  'doctor',\n",
       "  'door',\n",
       "  'doors',\n",
       "  'dora',\n",
       "  'doubt',\n",
       "  'draw',\n",
       "  'dread',\n",
       "  'dreadful',\n",
       "  'dream',\n",
       "  'dress',\n",
       "  'drink',\n",
       "  'drive',\n",
       "  'drop',\n",
       "  'dull',\n",
       "  'duty',\n",
       "  'early',\n",
       "  'ears',\n",
       "  'earth',\n",
       "  'ease',\n",
       "  'easily',\n",
       "  'easy',\n",
       "  'effect',\n",
       "  'elder',\n",
       "  'elliot',\n",
       "  'emotion',\n",
       "  'endeavour',\n",
       "  'engage',\n",
       "  'england',\n",
       "  'english',\n",
       "  'enjoy',\n",
       "  'enter',\n",
       "  'entirely',\n",
       "  'equal',\n",
       "  'escape',\n",
       "  'especially',\n",
       "  'events',\n",
       "  'everybody',\n",
       "  'evidence',\n",
       "  'evident',\n",
       "  'evidently',\n",
       "  'evil',\n",
       "  'exactly',\n",
       "  'examine',\n",
       "  'exceedingly',\n",
       "  'excellent',\n",
       "  'excite',\n",
       "  'excitement',\n",
       "  'exclaim',\n",
       "  'excuse',\n",
       "  'expect',\n",
       "  'experience',\n",
       "  'explain',\n",
       "  'explanation',\n",
       "  'express',\n",
       "  'expression',\n",
       "  'extraordinary',\n",
       "  'extreme',\n",
       "  'extremely',\n",
       "  'face',\n",
       "  'fact',\n",
       "  'facts',\n",
       "  'fail',\n",
       "  'faint',\n",
       "  'fair',\n",
       "  'faith',\n",
       "  'fall',\n",
       "  'family',\n",
       "  'fancy',\n",
       "  'farther',\n",
       "  'fashion',\n",
       "  'fast',\n",
       "  'fate',\n",
       "  'father',\n",
       "  'fault',\n",
       "  'favour',\n",
       "  'fear',\n",
       "  'feature',\n",
       "  'feel',\n",
       "  'feet',\n",
       "  'fell',\n",
       "  'fellow',\n",
       "  'felt',\n",
       "  'field',\n",
       "  'fight',\n",
       "  'figure',\n",
       "  'finally',\n",
       "  'fine',\n",
       "  'finger',\n",
       "  'finish',\n",
       "  'flash',\n",
       "  'fling',\n",
       "  'floor',\n",
       "  'flush',\n",
       "  'fodin',\n",
       "  'follow',\n",
       "  'fond',\n",
       "  'fool',\n",
       "  'foot',\n",
       "  'force',\n",
       "  'forget',\n",
       "  'forgive',\n",
       "  'form',\n",
       "  'forth',\n",
       "  'fortune',\n",
       "  'forward',\n",
       "  'free',\n",
       "  'french',\n",
       "  'fresh',\n",
       "  'friend',\n",
       "  'friendly',\n",
       "  'friends',\n",
       "  'frighten',\n",
       "  'future',\n",
       "  'gain',\n",
       "  'game',\n",
       "  'garden',\n",
       "  'gate',\n",
       "  'gather',\n",
       "  'gaze',\n",
       "  'general',\n",
       "  'generally',\n",
       "  'gentle',\n",
       "  'gentleman',\n",
       "  'gentlemen',\n",
       "  'girl',\n",
       "  'girls',\n",
       "  'glad',\n",
       "  'glance',\n",
       "  'glass',\n",
       "  'gold',\n",
       "  'good',\n",
       "  'grave',\n",
       "  'great',\n",
       "  'greater',\n",
       "  'greatest',\n",
       "  'greatly',\n",
       "  'green',\n",
       "  'grey',\n",
       "  'grind',\n",
       "  'grow',\n",
       "  'guard',\n",
       "  'guess',\n",
       "  'habit',\n",
       "  'hair',\n",
       "  'half',\n",
       "  'hall',\n",
       "  'hand',\n",
       "  'handsome',\n",
       "  'hang',\n",
       "  'happen',\n",
       "  'happiness',\n",
       "  'happy',\n",
       "  'hard',\n",
       "  'hardly',\n",
       "  'harm',\n",
       "  'harry',\n",
       "  'haste',\n",
       "  'hath',\n",
       "  'head',\n",
       "  'health',\n",
       "  'hear',\n",
       "  'heart',\n",
       "  'heaven',\n",
       "  'heavy',\n",
       "  'help',\n",
       "  'hide',\n",
       "  'high',\n",
       "  'highly',\n",
       "  'hill',\n",
       "  'hint',\n",
       "  'history',\n",
       "  'hold',\n",
       "  'home',\n",
       "  'honest',\n",
       "  'honour',\n",
       "  'hope',\n",
       "  'horror',\n",
       "  'horse',\n",
       "  'hour',\n",
       "  'hours',\n",
       "  'house',\n",
       "  'huge',\n",
       "  'human',\n",
       "  'humour',\n",
       "  'hurry',\n",
       "  'hurt',\n",
       "  'husband',\n",
       "  'idea',\n",
       "  'ideas',\n",
       "  'ill',\n",
       "  'imagine',\n",
       "  'immediately',\n",
       "  'importance',\n",
       "  'important',\n",
       "  'impossible',\n",
       "  'impression',\n",
       "  'increase',\n",
       "  'influence',\n",
       "  'inform',\n",
       "  'information',\n",
       "  'innocent',\n",
       "  'inquire',\n",
       "  'inspector',\n",
       "  'instance',\n",
       "  'instant',\n",
       "  'instantly',\n",
       "  'instead',\n",
       "  'insult',\n",
       "  'intend',\n",
       "  'intention',\n",
       "  'interrupt',\n",
       "  'introduce',\n",
       "  'invite',\n",
       "  'iron',\n",
       "  'ivan',\n",
       "  'jack',\n",
       "  'jane',\n",
       "  'join',\n",
       "  'joke',\n",
       "  'journey',\n",
       "  'judge',\n",
       "  'jump',\n",
       "  'just',\n",
       "  'justice',\n",
       "  'kill',\n",
       "  'kind',\n",
       "  'kindly',\n",
       "  'kindness',\n",
       "  'king',\n",
       "  'kiss',\n",
       "  'kitchen',\n",
       "  'knees',\n",
       "  'knife',\n",
       "  'knight',\n",
       "  'knock',\n",
       "  'know',\n",
       "  'knowledge',\n",
       "  'ladies',\n",
       "  'lady',\n",
       "  'lamp',\n",
       "  'land',\n",
       "  'large',\n",
       "  'late',\n",
       "  'later',\n",
       "  'laugh',\n",
       "  'laughter',\n",
       "  'lawyer',\n",
       "  'lead',\n",
       "  'lean',\n",
       "  'leap',\n",
       "  'learn',\n",
       "  'leave',\n",
       "  'legs',\n",
       "  'length',\n",
       "  'letter',\n",
       "  'life',\n",
       "  'lift',\n",
       "  'light',\n",
       "  'like',\n",
       "  'likely',\n",
       "  'line',\n",
       "  'lips',\n",
       "  'listen',\n",
       "  'little',\n",
       "  'live',\n",
       "  'lock',\n",
       "  'lodge',\n",
       "  'london',\n",
       "  'long',\n",
       "  'longer',\n",
       "  'look',\n",
       "  'looking',\n",
       "  'lord',\n",
       "  'lose',\n",
       "  'loss',\n",
       "  'loud',\n",
       "  'love',\n",
       "  'lower',\n",
       "  'madam',\n",
       "  'madame',\n",
       "  'maid',\n",
       "  'make',\n",
       "  'manage',\n",
       "  'manner',\n",
       "  'manners',\n",
       "  'mark',\n",
       "  'marriage',\n",
       "  'marry',\n",
       "  'master',\n",
       "  'match',\n",
       "  'matter',\n",
       "  'mean',\n",
       "  'measure',\n",
       "  'meet',\n",
       "  'memory',\n",
       "  'mention',\n",
       "  'mere',\n",
       "  'merely',\n",
       "  'message',\n",
       "  'middle',\n",
       "  'miles',\n",
       "  'mind',\n",
       "  'minute',\n",
       "  'minutes',\n",
       "  'miss',\n",
       "  'mistake',\n",
       "  'moment',\n",
       "  'moments',\n",
       "  'money',\n",
       "  'month',\n",
       "  'months',\n",
       "  'moor',\n",
       "  'morning',\n",
       "  'morrow',\n",
       "  'mother',\n",
       "  'mouth',\n",
       "  'mrs',\n",
       "  'murder',\n",
       "  'murmur',\n",
       "  'mutter',\n",
       "  'mystery',\n",
       "  'narrow',\n",
       "  'natural',\n",
       "  'nature',\n",
       "  'near',\n",
       "  'nearer',\n",
       "  'nearly',\n",
       "  'necessary',\n",
       "  'neck',\n",
       "  'need',\n",
       "  'neighbour',\n",
       "  'neighbourhood',\n",
       "  'nervous',\n",
       "  'news',\n",
       "  'nigel',\n",
       "  'night',\n",
       "  'noble',\n",
       "  'noise',\n",
       "  'nonsense',\n",
       "  'nose',\n",
       "  'note',\n",
       "  'notice',\n",
       "  'number',\n",
       "  'object',\n",
       "  'oblige',\n",
       "  'observe',\n",
       "  'occasion',\n",
       "  'occupy',\n",
       "  'occur',\n",
       "  'odin',\n",
       "  'odins',\n",
       "  'offer',\n",
       "  'office',\n",
       "  'officer',\n",
       "  'open',\n",
       "  'opinion',\n",
       "  'opportunity',\n",
       "  'opposite',\n",
       "  'order',\n",
       "  'ought',\n",
       "  'outside',\n",
       "  'pace',\n",
       "  'pain',\n",
       "  'painful',\n",
       "  'pair',\n",
       "  'pale',\n",
       "  'paper',\n",
       "  'pardon',\n",
       "  'park',\n",
       "  'particular',\n",
       "  'particularly',\n",
       "  'party',\n",
       "  'pass',\n",
       "  'passage',\n",
       "  'past',\n",
       "  'path',\n",
       "  'pause',\n",
       "  'peace',\n",
       "  'peculiar',\n",
       "  'people',\n",
       "  'perceive',\n",
       "  'perfect',\n",
       "  'perfectly',\n",
       "  'person',\n",
       "  'persuade',\n",
       "  'pick',\n",
       "  'picture',\n",
       "  'piece',\n",
       "  'pipe',\n",
       "  'pity',\n",
       "  'place',\n",
       "  'plain',\n",
       "  'plainly',\n",
       "  'plan',\n",
       "  'play',\n",
       "  'pleasant',\n",
       "  'pleasure',\n",
       "  'point',\n",
       "  'police',\n",
       "  'poor',\n",
       "  'position',\n",
       "  'positively',\n",
       "  'possible',\n",
       "  'possibly',\n",
       "  'post',\n",
       "  'pound',\n",
       "  'power',\n",
       "  'praise',\n",
       "  'pray',\n",
       "  'prefer',\n",
       "  'prepare',\n",
       "  'presence',\n",
       "  'present',\n",
       "  'presently',\n",
       "  'press',\n",
       "  'pretend',\n",
       "  'pretty',\n",
       "  'prevent',\n",
       "  'pride',\n",
       "  'prince',\n",
       "  'prison',\n",
       "  'prisoner',\n",
       "  'private',\n",
       "  'probably',\n",
       "  'proceed',\n",
       "  'produce',\n",
       "  'professor',\n",
       "  'promise',\n",
       "  'proof',\n",
       "  'proper',\n",
       "  'propose',\n",
       "  'proud',\n",
       "  'prove',\n",
       "  'public',\n",
       "  'pull',\n",
       "  'purpose',\n",
       "  'pursue',\n",
       "  'push',\n",
       "  'quarrel',\n",
       "  'quarter',\n",
       "  'question',\n",
       "  'quick',\n",
       "  'quickly',\n",
       "  'quiet',\n",
       "  'quietly',\n",
       "  'quite',\n",
       "  'rain',\n",
       "  'raise',\n",
       "  'reach',\n",
       "  'read',\n",
       "  'ready',\n",
       "  'real',\n",
       "  'really',\n",
       "  'reason',\n",
       "  'recall',\n",
       "  'receive',\n",
       "  'reckon',\n",
       "  'recognise',\n",
       "  'recollect',\n",
       "  'recover',\n",
       "  'reflect',\n",
       "  'refuse',\n",
       "  'regard',\n",
       "  'regret',\n",
       "  'remain',\n",
       "  'remark',\n",
       "  'remarkable',\n",
       "  'remember',\n",
       "  'remove',\n",
       "  'repeat',\n",
       "  'reply',\n",
       "  'report',\n",
       "  'request',\n",
       "  'require',\n",
       "  'resolve',\n",
       "  'respect',\n",
       "  'rest',\n",
       "  'result',\n",
       "  'resume',\n",
       "  'retire',\n",
       "  'return',\n",
       "  'rich',\n",
       "  'ride',\n",
       "  'right',\n",
       "  'ring',\n",
       "  'rise',\n",
       "  'river',\n",
       "  'road',\n",
       "  'roar',\n",
       "  'rock',\n",
       "  'roll',\n",
       "  'roof',\n",
       "  'room',\n",
       "  'roubles',\n",
       "  'rough',\n",
       "  'round',\n",
       "  'ruin',\n",
       "  'rush',\n",
       "  'russian',\n",
       "  'safe',\n",
       "  'saint',\n",
       "  'sake',\n",
       "  'satisfaction',\n",
       "  'satisfy',\n",
       "  'save',\n",
       "  'scarcely',\n",
       "  'scene',\n",
       "  'school',\n",
       "  'scream',\n",
       "  'search',\n",
       "  'seat',\n",
       "  'second',\n",
       "  'secret',\n",
       "  'seek',\n",
       "  'seize',\n",
       "  'self',\n",
       "  'send',\n",
       "  'sense',\n",
       "  'sensible',\n",
       "  'seriously',\n",
       "  'servant',\n",
       "  'servants',\n",
       "  'serve',\n",
       "  'service',\n",
       "  'settle',\n",
       "  'seven',\n",
       "  'shadow',\n",
       "  'shake',\n",
       "  'shall',\n",
       "  'shame',\n",
       "  'share',\n",
       "  'sharp',\n",
       "  'shin',\n",
       "  'ship',\n",
       "  'shock',\n",
       "  'shoot',\n",
       "  'shop',\n",
       "  'short',\n",
       "  'shoulder',\n",
       "  'shout',\n",
       "  'shut',\n",
       "  'sick',\n",
       "  'sigh',\n",
       "  'sight',\n",
       "  'sign',\n",
       "  'silence',\n",
       "  'silent',\n",
       "  'silver',\n",
       "  'simple',\n",
       "  'simply',\n",
       "  'sing',\n",
       "  'single',\n",
       "  'singular',\n",
       "  'sink',\n",
       "  'sister',\n",
       "  'sisters',\n",
       "  'situation',\n",
       "  'sleep',\n",
       "  'slight',\n",
       "  'slip',\n",
       "  'slowly',\n",
       "  'small',\n",
       "  'smile',\n",
       "  'smith',\n",
       "  'smoke',\n",
       "  'snow',\n",
       "  'society',\n",
       "  'sofa',\n",
       "  'softly',\n",
       "  'soldier',\n",
       "  'somebody',\n",
       "  'somewhat',\n",
       "  'soon',\n",
       "  'sooner',\n",
       "  'sorry',\n",
       "  'sort',\n",
       "  'soul',\n",
       "  'sound',\n",
       "  'spare',\n",
       "  'speak',\n",
       "  'special',\n",
       "  'speech',\n",
       "  'spend',\n",
       "  'spirit',\n",
       "  'spite',\n",
       "  'spot',\n",
       "  'spring',\n",
       "  'squire',\n",
       "  'stairs',\n",
       "  'stand',\n",
       "  'star',\n",
       "  'start',\n",
       "  'state',\n",
       "  'station',\n",
       "  'stay',\n",
       "  'steal',\n",
       "  'step',\n",
       "  'stepan',\n",
       "  'stick',\n",
       "  'stir',\n",
       "  'stone',\n",
       "  'stop',\n",
       "  'story',\n",
       "  'straight',\n",
       "  'strange',\n",
       "  'stranger',\n",
       "  'street',\n",
       "  'streets',\n",
       "  'strength',\n",
       "  'stretch',\n",
       "  'strike',\n",
       "  'strong',\n",
       "  'struggle',\n",
       "  'study',\n",
       "  'stupid',\n",
       "  'subject',\n",
       "  'succeed',\n",
       "  'sudden',\n",
       "  'suddenly',\n",
       "  'suffer',\n",
       "  'suggest',\n",
       "  'suit',\n",
       "  'support',\n",
       "  'suppose',\n",
       "  'sure',\n",
       "  'surely',\n",
       "  'surprise',\n",
       "  'surround',\n",
       "  'suspect',\n",
       "  'suspicion',\n",
       "  'swear',\n",
       "  'sweet',\n",
       "  'table',\n",
       "  'talk',\n",
       "  'tall',\n",
       "  'taste',\n",
       "  'teach',\n",
       "  'tear',\n",
       "  'teeth',\n",
       "  'tell',\n",
       "  'temper',\n",
       "  'term',\n",
       "  'terrible',\n",
       "  'terror',\n",
       "  'thank',\n",
       "  'thing',\n",
       "  'things',\n",
       "  'think',\n",
       "  'thirty',\n",
       "  'thou',\n",
       "  'thoughts',\n",
       "  'thousand',\n",
       "  'throat',\n",
       "  'throw',\n",
       "  'thrust',\n",
       "  'till',\n",
       "  'time',\n",
       "  'tire',\n",
       "  'tone',\n",
       "  'tongue',\n",
       "  'touch',\n",
       "  'town',\n",
       "  'trace',\n",
       "  'train',\n",
       "  'travel',\n",
       "  'treasure',\n",
       "  'treat',\n",
       "  'tree',\n",
       "  'tremble',\n",
       "  'trouble',\n",
       "  'true',\n",
       "  'trust',\n",
       "  'truth',\n",
       "  'turn',\n",
       "  'twice',\n",
       "  'unable',\n",
       "  'uncle',\n",
       "  'understand',\n",
       "  'uneasy',\n",
       "  'unhappy',\n",
       "  'unless',\n",
       "  'upstairs',\n",
       "  'uriah',\n",
       "  'usual',\n",
       "  'utter',\n",
       "  'vain',\n",
       "  'value',\n",
       "  'venture',\n",
       "  'view',\n",
       "  'village',\n",
       "  'violent',\n",
       "  'visit',\n",
       "  'visitor',\n",
       "  'voice',\n",
       "  'wait',\n",
       "  'wake',\n",
       "  'walk',\n",
       "  'wall',\n",
       "  'wander',\n",
       "  'want',\n",
       "  'warm',\n",
       "  'warn',\n",
       "  'watch',\n",
       "  'water',\n",
       "  'wave',\n",
       "  'ways',\n",
       "  'weak',\n",
       "  'wear',\n",
       "  'weary',\n",
       "  'weather',\n",
       "  'week',\n",
       "  'weeks',\n",
       "  'weep',\n",
       "  'welcome',\n",
       "  'whisper',\n",
       "  'white',\n",
       "  'wide',\n",
       "  'wife',\n",
       "  'wild',\n",
       "  'wind',\n",
       "  'window',\n",
       "  'windows',\n",
       "  'wine',\n",
       "  'wish',\n",
       "  'witness',\n",
       "  'woman',\n",
       "  'women',\n",
       "  'wonder',\n",
       "  'wonderful',\n",
       "  'wood',\n",
       "  'wooden',\n",
       "  'word',\n",
       "  'work',\n",
       "  'world',\n",
       "  'worse',\n",
       "  'worst',\n",
       "  'worth',\n",
       "  'worthy',\n",
       "  'write',\n",
       "  'wrong',\n",
       "  'yard',\n",
       "  'year',\n",
       "  'years',\n",
       "  'yellow',\n",
       "  'yesterday',\n",
       "  'young',\n",
       "  'youth'])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(terms), terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T11:58:05.008359Z",
     "start_time": "2020-11-02T11:58:05.004538Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_authors(components, feature_names, n=5):\n",
    "    for idx, author in enumerate(components):\n",
    "        print(\"Author %d :\" % (idx+1), [(feature_names[i], author[i].round(2)) for i in author.argsort()[:-n -1:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T11:58:39.193003Z",
     "start_time": "2020-11-02T11:58:39.188358Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author 1 : [('answer', 714.33), ('odin', 488.61), ('father', 426.84), ('miss', 292.31), ('poor', 263.17)]\n",
      "Author 2 : [('odin', 1137.78), ('good', 547.16), ('make', 536.15), ('time', 477.73), ('mean', 476.11)]\n",
      "Author 3 : [('odin', 746.35), ('look', 466.62), ('laugh', 402.7), ('shall', 371.47), ('face', 355.08)]\n",
      "Author 4 : [('odin', 686.84), ('reply', 443.68), ('hand', 429.69), ('voice', 380.91), ('head', 355.9)]\n",
      "Author 5 : [('odin', 2241.94), ('know', 689.76), ('think', 682.12), ('come', 562.1), ('right', 462.01)]\n"
     ]
    }
   ],
   "source": [
    "get_authors(lda_model.components_, terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max_features 제한 없이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T12:03:43.601461Z",
     "start_time": "2020-11-02T12:03:42.722877Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(text['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T12:03:43.606425Z",
     "start_time": "2020-11-02T12:03:43.603411Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54879, 28005)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T12:15:26.599819Z",
     "start_time": "2020-11-02T12:15:26.560892Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28005,\n",
       " ['000',\n",
       "  '10',\n",
       "  '100',\n",
       "  '1000',\n",
       "  '10_s_',\n",
       "  '11',\n",
       "  '114th',\n",
       "  '126b',\n",
       "  '12_s_',\n",
       "  '12th',\n",
       "  '13th',\n",
       "  '14',\n",
       "  '1429',\n",
       "  '1456',\n",
       "  '146m',\n",
       "  '14th',\n",
       "  '15',\n",
       "  '15_th',\n",
       "  '15th',\n",
       "  '1647',\n",
       "  '1676',\n",
       "  '16th',\n",
       "  '1715',\n",
       "  '1733',\n",
       "  '1742',\n",
       "  '1745',\n",
       "  '1748',\n",
       "  '1749',\n",
       "  '1750',\n",
       "  '1751',\n",
       "  '1756',\n",
       "  '1757',\n",
       "  '1764',\n",
       "  '1767',\n",
       "  '1772',\n",
       "  '1792',\n",
       "  '17__',\n",
       "  '17_th_',\n",
       "  '1803',\n",
       "  '1810',\n",
       "  '1812',\n",
       "  '1814',\n",
       "  '1820',\n",
       "  '1826',\n",
       "  '1830',\n",
       "  '1840',\n",
       "  '1855',\n",
       "  '1856',\n",
       "  '1859',\n",
       "  '1860',\n",
       "  '1861',\n",
       "  '1862',\n",
       "  '1865',\n",
       "  '1869',\n",
       "  '1870',\n",
       "  '1874',\n",
       "  '1875',\n",
       "  '1876',\n",
       "  '1878',\n",
       "  '1882',\n",
       "  '1883',\n",
       "  '1884',\n",
       "  '1887',\n",
       "  '1888',\n",
       "  '1890',\n",
       "  '1891',\n",
       "  '1894',\n",
       "  '1895',\n",
       "  '1898',\n",
       "  '18th',\n",
       "  '1908',\n",
       "  '1914',\n",
       "  '1_s_',\n",
       "  '20',\n",
       "  '200',\n",
       "  '21st',\n",
       "  '221b',\n",
       "  '22nd',\n",
       "  '23rd',\n",
       "  '2473',\n",
       "  '249',\n",
       "  '26th',\n",
       "  '2704',\n",
       "  '28th',\n",
       "  '29th',\n",
       "  '2_s_',\n",
       "  '2d',\n",
       "  '30',\n",
       "  '303',\n",
       "  '34th',\n",
       "  '35',\n",
       "  '37',\n",
       "  '40',\n",
       "  '4000',\n",
       "  '421',\n",
       "  '470',\n",
       "  '50',\n",
       "  '500',\n",
       "  '52',\n",
       "  '6_d_',\n",
       "  '6th',\n",
       "  '7000l',\n",
       "  '7_s_',\n",
       "  '89',\n",
       "  '8_d_',\n",
       "  '8_s_',\n",
       "  '97163',\n",
       "  '_absolutely_',\n",
       "  '_adair_',\n",
       "  '_adieu',\n",
       "  '_advantages_',\n",
       "  '_affect_',\n",
       "  '_afraid_',\n",
       "  '_after',\n",
       "  '_after_',\n",
       "  '_afterwards',\n",
       "  '_age_',\n",
       "  '_aimais_',\n",
       "  '_air_',\n",
       "  '_alea',\n",
       "  '_all',\n",
       "  '_all_',\n",
       "  '_allow_',\n",
       "  '_almost_',\n",
       "  '_alone_',\n",
       "  '_already_',\n",
       "  '_always_',\n",
       "  '_am_',\n",
       "  '_american',\n",
       "  '_amor_',\n",
       "  '_amore_',\n",
       "  '_and',\n",
       "  '_and_',\n",
       "  '_answer_',\n",
       "  '_any_',\n",
       "  '_appearance_',\n",
       "  '_appropriation_',\n",
       "  '_appulyaird',\n",
       "  '_après',\n",
       "  '_are',\n",
       "  '_are_',\n",
       "  '_arrière',\n",
       "  '_art_',\n",
       "  '_assez',\n",
       "  '_at_',\n",
       "  '_athens',\n",
       "  '_auntie',\n",
       "  '_aurora_',\n",
       "  '_authority_',\n",
       "  '_auto',\n",
       "  '_avec',\n",
       "  '_awful_',\n",
       "  '_bah',\n",
       "  '_be',\n",
       "  '_be_',\n",
       "  '_bear_',\n",
       "  '_beaux',\n",
       "  '_because',\n",
       "  '_been_',\n",
       "  '_before_',\n",
       "  '_begin_',\n",
       "  '_being',\n",
       "  '_being_',\n",
       "  '_believe_',\n",
       "  '_believes',\n",
       "  '_bid',\n",
       "  '_bid_',\n",
       "  '_bienfait_',\n",
       "  '_bigre_',\n",
       "  '_black',\n",
       "  '_blunder_',\n",
       "  '_boiled_',\n",
       "  '_bon_',\n",
       "  '_bonjour_',\n",
       "  '_bonâ',\n",
       "  '_both',\n",
       "  '_both_',\n",
       "  '_boulanger_',\n",
       "  '_bourru',\n",
       "  '_brave',\n",
       "  '_british',\n",
       "  '_broke_',\n",
       "  '_brother_',\n",
       "  '_brune_',\n",
       "  '_but',\n",
       "  '_but_',\n",
       "  '_c',\n",
       "  '_cabaret_',\n",
       "  '_cafes_',\n",
       "  '_called_',\n",
       "  '_can',\n",
       "  '_can_',\n",
       "  '_candide_',\n",
       "  '_cannot_',\n",
       "  '_carbonari_',\n",
       "  '_carte',\n",
       "  '_cas',\n",
       "  '_cause_',\n",
       "  '_cavendish',\n",
       "  '_cela',\n",
       "  '_certain_',\n",
       "  '_certainty_',\n",
       "  '_ces',\n",
       "  '_cette',\n",
       "  '_chaperon_',\n",
       "  '_charming_',\n",
       "  '_cher',\n",
       "  '_cher_',\n",
       "  '_chevalier_',\n",
       "  '_chevaux',\n",
       "  '_chez',\n",
       "  '_child_',\n",
       "  '_christ',\n",
       "  '_christian',\n",
       "  '_chronicle_',\n",
       "  '_chère',\n",
       "  '_chère_',\n",
       "  '_cinq',\n",
       "  '_come_',\n",
       "  '_command',\n",
       "  '_comment',\n",
       "  '_commonplace_',\n",
       "  '_community_',\n",
       "  '_compassion_',\n",
       "  '_compelled_',\n",
       "  '_complete_',\n",
       "  '_compliments_',\n",
       "  '_compote_',\n",
       "  '_con_',\n",
       "  '_conditionally_',\n",
       "  '_confrere_',\n",
       "  '_confrère_',\n",
       "  '_conqueror_',\n",
       "  '_conscious_',\n",
       "  '_continued_',\n",
       "  '_contretemps_',\n",
       "  '_corps_',\n",
       "  '_could',\n",
       "  '_could_',\n",
       "  '_count_',\n",
       "  '_coup',\n",
       "  '_courtship_',\n",
       "  '_credo_',\n",
       "  '_crevez',\n",
       "  '_cried_',\n",
       "  '_curse_',\n",
       "  '_cuts',\n",
       "  '_daily',\n",
       "  '_damn_',\n",
       "  '_dans',\n",
       "  '_dare_',\n",
       "  '_daughters_',\n",
       "  '_dead_',\n",
       "  '_deepest_',\n",
       "  '_defiant',\n",
       "  '_delirium_',\n",
       "  '_deus',\n",
       "  '_did',\n",
       "  '_did_',\n",
       "  '_didn',\n",
       "  '_die',\n",
       "  '_dieux',\n",
       "  '_dined_',\n",
       "  '_dines_',\n",
       "  '_dio',\n",
       "  '_discourteous_',\n",
       "  '_disjecta',\n",
       "  '_dislike_',\n",
       "  '_dissolved_',\n",
       "  '_distrait_',\n",
       "  '_dixons_',\n",
       "  '_do_',\n",
       "  '_doctor',\n",
       "  '_does_',\n",
       "  '_don',\n",
       "  '_double_',\n",
       "  '_doubt_',\n",
       "  '_doubts_',\n",
       "  '_drap',\n",
       "  '_dreams_',\n",
       "  '_du_',\n",
       "  '_débris_',\n",
       "  '_dénouement_',\n",
       "  '_déshabillé_',\n",
       "  '_each_',\n",
       "  '_echo_',\n",
       "  '_eclaircissement_',\n",
       "  '_einen_',\n",
       "  '_eldest_',\n",
       "  '_eliminate_',\n",
       "  '_elle',\n",
       "  '_eloignez',\n",
       "  '_embarras',\n",
       "  '_employé_',\n",
       "  '_encyclopædia',\n",
       "  '_encyclopædia_',\n",
       "  '_ends_',\n",
       "  '_enfin',\n",
       "  '_engaged_',\n",
       "  '_engagement_',\n",
       "  '_ensemble_',\n",
       "  '_erect',\n",
       "  '_esmeralda_',\n",
       "  '_espirito',\n",
       "  '_esprit_',\n",
       "  '_etourderie_',\n",
       "  '_etruria_',\n",
       "  '_even',\n",
       "  '_evening',\n",
       "  '_evenings',\n",
       "  '_everything_',\n",
       "  '_excellent',\n",
       "  '_exigeant_',\n",
       "  '_existe',\n",
       "  '_experiment_',\n",
       "  '_extracted',\n",
       "  '_face_',\n",
       "  '_facts_',\n",
       "  '_fait',\n",
       "  '_familiar_',\n",
       "  '_farewell_',\n",
       "  '_father',\n",
       "  '_feigned_',\n",
       "  '_felt_',\n",
       "  '_few_',\n",
       "  '_fey_',\n",
       "  '_fiancé_',\n",
       "  '_fiancée_',\n",
       "  '_financial',\n",
       "  '_first_',\n",
       "  '_firstly_',\n",
       "  '_following',\n",
       "  '_folly_',\n",
       "  '_food_',\n",
       "  '_for',\n",
       "  '_force_',\n",
       "  '_forgets_',\n",
       "  '_forgiven_',\n",
       "  '_forgot',\n",
       "  '_fortnightly',\n",
       "  '_fougue',\n",
       "  '_foundation_',\n",
       "  '_four_',\n",
       "  '_fracas_',\n",
       "  '_fraternité',\n",
       "  '_free_',\n",
       "  '_from',\n",
       "  '_fête_',\n",
       "  '_gamecock_',\n",
       "  '_ganz_',\n",
       "  '_gatzuk_',\n",
       "  '_general_',\n",
       "  '_geological',\n",
       "  '_gigantic_',\n",
       "  '_glad_',\n",
       "  '_globe_',\n",
       "  '_gloire',\n",
       "  '_gloria',\n",
       "  '_god',\n",
       "  '_god_',\n",
       "  '_going_',\n",
       "  '_gold_',\n",
       "  '_good',\n",
       "  '_good_',\n",
       "  '_gossip_',\n",
       "  '_gott',\n",
       "  '_grace',\n",
       "  '_greater_',\n",
       "  '_had',\n",
       "  '_had_',\n",
       "  '_hairry',\n",
       "  '_half_',\n",
       "  '_hamlet_',\n",
       "  '_hand_',\n",
       "  '_happy_',\n",
       "  '_has_',\n",
       "  '_hate_',\n",
       "  '_have_',\n",
       "  '_he_',\n",
       "  '_heard_',\n",
       "  '_hein',\n",
       "  '_hein_',\n",
       "  '_help_',\n",
       "  '_her',\n",
       "  '_her_',\n",
       "  '_here_',\n",
       "  '_hers_',\n",
       "  '_herself',\n",
       "  '_him',\n",
       "  '_him_',\n",
       "  '_him_self',\n",
       "  '_his',\n",
       "  '_his_',\n",
       "  '_home_',\n",
       "  '_hominibus',\n",
       "  '_hopes_',\n",
       "  '_horrible_',\n",
       "  '_horror_',\n",
       "  '_hotspur_',\n",
       "  '_housebreaking_',\n",
       "  '_how',\n",
       "  '_how_',\n",
       "  '_however',\n",
       "  '_hundred_',\n",
       "  '_hushed_',\n",
       "  '_i',\n",
       "  '_i_',\n",
       "  '_ici_',\n",
       "  '_if_',\n",
       "  '_impressed_',\n",
       "  '_in_',\n",
       "  '_index',\n",
       "  '_ingratitude_',\n",
       "  '_inseparable_',\n",
       "  '_insist_',\n",
       "  '_interest_',\n",
       "  '_interesting_',\n",
       "  '_internationale',\n",
       "  '_is_',\n",
       "  '_island_',\n",
       "  '_it_',\n",
       "  '_italia',\n",
       "  '_j',\n",
       "  '_jealous_',\n",
       "  '_jesus',\n",
       "  '_journal',\n",
       "  '_julie',\n",
       "  '_july_',\n",
       "  '_just_',\n",
       "  '_kammerherr',\n",
       "  '_kammerherr_',\n",
       "  '_kammerherrs_',\n",
       "  '_keenly_',\n",
       "  '_knew_',\n",
       "  '_know_',\n",
       "  '_knows_',\n",
       "  '_knyght_',\n",
       "  '_kratides',\n",
       "  '_l',\n",
       "  '_la_',\n",
       "  '_lady_',\n",
       "  '_lajdak',\n",
       "  '_lajdak_',\n",
       "  '_lancet_',\n",
       "  '_last',\n",
       "  '_last_',\n",
       "  '_laugh_',\n",
       "  '_learned',\n",
       "  '_lepidus',\n",
       "  '_les',\n",
       "  '_less_',\n",
       "  '_let_',\n",
       "  '_letter',\n",
       "  '_letting_',\n",
       "  '_lever',\n",
       "  '_libertas_',\n",
       "  '_like',\n",
       "  '_like_',\n",
       "  '_listening_',\n",
       "  '_little_',\n",
       "  '_lives',\n",
       "  '_living_',\n",
       "  '_lone',\n",
       "  '_long',\n",
       "  '_look_',\n",
       "  '_lord_',\n",
       "  '_lourdeau_',\n",
       "  '_love_',\n",
       "  '_madonna_',\n",
       "  '_maire',\n",
       "  '_mais',\n",
       "  '_manner_',\n",
       "  '_many_',\n",
       "  '_marchand_',\n",
       "  '_maria',\n",
       "  '_marlborough',\n",
       "  '_marry_',\n",
       "  '_master_',\n",
       "  '_maternal_',\n",
       "  '_matilda_',\n",
       "  '_may',\n",
       "  '_may_',\n",
       "  '_maître_',\n",
       "  '_me_',\n",
       "  '_mean_',\n",
       "  '_meaning_',\n",
       "  '_meant_',\n",
       "  '_mediocre_',\n",
       "  '_memento',\n",
       "  '_menus_',\n",
       "  '_merci',\n",
       "  '_merci_',\n",
       "  '_mere',\n",
       "  '_mes',\n",
       "  '_messieurs',\n",
       "  '_might_',\n",
       "  '_mine',\n",
       "  '_mine_',\n",
       "  '_miracle_',\n",
       "  '_miserable_',\n",
       "  '_misery_',\n",
       "  '_miss',\n",
       "  '_miss_',\n",
       "  '_missish_',\n",
       "  '_moi',\n",
       "  '_mon',\n",
       "  '_monday',\n",
       "  '_month_',\n",
       "  '_more',\n",
       "  '_more_',\n",
       "  '_morning',\n",
       "  '_mortal_',\n",
       "  '_most_',\n",
       "  '_motives_',\n",
       "  '_mr',\n",
       "  '_mr_',\n",
       "  '_mrs',\n",
       "  '_much_',\n",
       "  '_must_',\n",
       "  '_my_',\n",
       "  '_my_self',\n",
       "  '_myself_',\n",
       "  '_mystery_',\n",
       "  '_métier_',\n",
       "  '_n',\n",
       "  '_name_',\n",
       "  '_names_',\n",
       "  '_naïveté_',\n",
       "  '_near_',\n",
       "  '_never_',\n",
       "  '_new_',\n",
       "  '_no_',\n",
       "  '_non',\n",
       "  '_none_',\n",
       "  '_nonesuch_',\n",
       "  '_nonsense',\n",
       "  '_norval_',\n",
       "  '_not',\n",
       "  '_not_',\n",
       "  '_nothing_',\n",
       "  '_notre',\n",
       "  '_nous',\n",
       "  '_now',\n",
       "  '_now_',\n",
       "  '_née_',\n",
       "  '_october_',\n",
       "  '_odin',\n",
       "  '_odin_',\n",
       "  '_odinness_',\n",
       "  '_of_',\n",
       "  '_office_',\n",
       "  '_often_',\n",
       "  '_omne',\n",
       "  '_once_',\n",
       "  '_one_',\n",
       "  '_only_',\n",
       "  '_onyegin_',\n",
       "  '_orders_',\n",
       "  '_othello_',\n",
       "  '_ought_',\n",
       "  '_our_',\n",
       "  '_out_',\n",
       "  '_outré_',\n",
       "  '_ouvrier_',\n",
       "  '_own_',\n",
       "  '_pall',\n",
       "  '_pan',\n",
       "  '_pan_',\n",
       "  '_panem',\n",
       "  '_pani_',\n",
       "  '_panie',\n",
       "  '_panie_',\n",
       "  '_panienotchka_',\n",
       "  '_panovie',\n",
       "  '_panovie_',\n",
       "  '_par',\n",
       "  '_parc',\n",
       "  '_parce',\n",
       "  '_pardon',\n",
       "  '_part_',\n",
       "  '_particular_',\n",
       "  '_particularly_',\n",
       "  '_partie',\n",
       "  '_pas',\n",
       "  '_passons',\n",
       "  '_patriae_',\n",
       "  '_pax',\n",
       "  '_performance_',\n",
       "  '_periodical',\n",
       "  '_periodical_',\n",
       "  '_peroratio_',\n",
       "  '_persuasion_',\n",
       "  '_petite',\n",
       "  '_pitied_',\n",
       "  '_pity_',\n",
       "  '_plaisirs_',\n",
       "  '_planning_',\n",
       "  '_pledge_',\n",
       "  '_port',\n",
       "  '_poseurs_',\n",
       "  '_possible_',\n",
       "  '_post',\n",
       "  '_pour',\n",
       "  '_practical',\n",
       "  '_present_',\n",
       "  '_presume_',\n",
       "  '_price_',\n",
       "  '_pride_',\n",
       "  '_pro',\n",
       "  '_promise_',\n",
       "  '_promised_',\n",
       "  '_propriety_',\n",
       "  '_propter',\n",
       "  '_protégé_',\n",
       "  '_protégés_',\n",
       "  '_prove_',\n",
       "  '_psychology_',\n",
       "  '_pâté',\n",
       "  '_qu',\n",
       "  '_quantum',\n",
       "  '_que',\n",
       "  '_quel',\n",
       "  '_quelle',\n",
       "  '_quelque',\n",
       "  '_quite_',\n",
       "  '_radix',\n",
       "  '_read',\n",
       "  '_read_',\n",
       "  '_real',\n",
       "  '_real_',\n",
       "  '_really_',\n",
       "  '_rears_',\n",
       "  '_reasonable_',\n",
       "  '_recherché_',\n",
       "  '_recollecting_',\n",
       "  '_red',\n",
       "  '_refuse_',\n",
       "  '_remind_',\n",
       "  '_repentance_',\n",
       "  '_reproach',\n",
       "  '_respect_',\n",
       "  '_reward',\n",
       "  '_rich_',\n",
       "  '_right_',\n",
       "  '_rigor',\n",
       "  '_risus',\n",
       "  '_robin_',\n",
       "  '_roof_',\n",
       "  '_roost_',\n",
       "  '_rouge',\n",
       "  '_russian_',\n",
       "  '_rôle_',\n",
       "  '_sacrifice_',\n",
       "  '_said_',\n",
       "  '_sainte',\n",
       "  '_salon_',\n",
       "  '_sans',\n",
       "  '_sarah_',\n",
       "  '_saw_',\n",
       "  '_say',\n",
       "  '_say_',\n",
       "  '_scaura_',\n",
       "  '_scena_',\n",
       "  '_scent',\n",
       "  '_scheme_',\n",
       "  '_second_',\n",
       "  '_secondly_',\n",
       "  '_secret_',\n",
       "  '_see_',\n",
       "  '_seemed_',\n",
       "  '_seems_',\n",
       "  '_sensation_',\n",
       "  '_sensible_',\n",
       "  '_sept_',\n",
       "  '_seriously_',\n",
       "  '_shadow_',\n",
       "  '_shall_',\n",
       "  '_she',\n",
       "  '_she_',\n",
       "  '_ship_',\n",
       "  '_shoes_',\n",
       "  '_should_',\n",
       "  '_shule',\n",
       "  '_signor_',\n",
       "  '_sine',\n",
       "  '_sir',\n",
       "  '_sir_',\n",
       "  '_six',\n",
       "  '_small_',\n",
       "  '_smoke',\n",
       "  '_so_',\n",
       "  '_some_',\n",
       "  '_something',\n",
       "  '_something_',\n",
       "  '_soon',\n",
       "  '_sophy',\n",
       "  '_sorrow',\n",
       "  '_source_',\n",
       "  '_staff_',\n",
       "  '_standard_',\n",
       "  '_star_',\n",
       "  '_such',\n",
       "  '_such_',\n",
       "  '_sum',\n",
       "  '_sì_',\n",
       "  '_table_',\n",
       "  '_tableaux',\n",
       "  '_taken_',\n",
       "  '_tant',\n",
       "  '_telegraph_',\n",
       "  '_temper_',\n",
       "  '_ten_',\n",
       "  '_tenez',\n",
       "  '_tete',\n",
       "  '_than_',\n",
       "  '_that',\n",
       "  '_that_',\n",
       "  '_the',\n",
       "  '_the_',\n",
       "  '_theatre_',\n",
       "  '_their_',\n",
       "  '_them',\n",
       "  '_them_',\n",
       "  '_then_',\n",
       "  '_theoretically_',\n",
       "  '_there',\n",
       "  '_there_',\n",
       "  '_these_',\n",
       "  '_they',\n",
       "  '_they_',\n",
       "  '_thing_',\n",
       "  '_think_',\n",
       "  '_third_',\n",
       "  '_thirdly_',\n",
       "  '_this',\n",
       "  '_this_',\n",
       "  '_those',\n",
       "  '_thoughts_',\n",
       "  '_three',\n",
       "  '_through_',\n",
       "  '_time_',\n",
       "  '_times_',\n",
       "  '_to_',\n",
       "  '_today',\n",
       "  '_today_',\n",
       "  '_told_',\n",
       "  '_tolerable_',\n",
       "  '_tone_',\n",
       "  '_too',\n",
       "  '_too_',\n",
       "  '_tour',\n",
       "  '_tout',\n",
       "  '_tout_',\n",
       "  '_traditore_',\n",
       "  '_trivial_',\n",
       "  '_two_',\n",
       "  '_twofold_',\n",
       "  '_tête',\n",
       "  '_unchristian',\n",
       "  '_uncle_',\n",
       "  '_understand',\n",
       "  '_une',\n",
       "  '_universal',\n",
       "  '_up_',\n",
       "  '_us_',\n",
       "  '_used_',\n",
       "  '_vater',\n",
       "  '_vater_',\n",
       "  '_very_',\n",
       "  '_vices_',\n",
       "  '_vieni_',\n",
       "  '_views_',\n",
       "  '_villainous_',\n",
       "  '_vingt',\n",
       "  '_virtuoso_',\n",
       "  '_vis',\n",
       "  '_vistula_',\n",
       "  '_vivant_',\n",
       "  '_vive',\n",
       "  '_vivos',\n",
       "  '_vogue',\n",
       "  '_voice_',\n",
       "  '_voilà',\n",
       "  '_vonsohn_',\n",
       "  '_voodooism',\n",
       "  '_vous',\n",
       "  '_voyez',\n",
       "  '_wandering',\n",
       "  '_want_',\n",
       "  '_was',\n",
       "  '_was_',\n",
       "  '_wastes_',\n",
       "  '_we_',\n",
       "  '_weekly',\n",
       "  '_were_',\n",
       "  '_western',\n",
       "  '_what',\n",
       "  '_what_',\n",
       "  '_when',\n",
       "  '_when_',\n",
       "  '_where',\n",
       "  '_where_',\n",
       "  '_wherein_',\n",
       "  '_while',\n",
       "  '_who',\n",
       "  '_who_',\n",
       "  '_why',\n",
       "  '_why_',\n",
       "  '_will_',\n",
       "  '_wish_',\n",
       "  '_wished_',\n",
       "  '_wishes_',\n",
       "  '_with',\n",
       "  '_within',\n",
       "  '_words_',\n",
       "  '_worse',\n",
       "  '_wos_',\n",
       "  '_would',\n",
       "  '_would_',\n",
       "  '_you',\n",
       "  '_you_',\n",
       "  '_younger_',\n",
       "  '_your',\n",
       "  '_your_',\n",
       "  '_yours_',\n",
       "  '_yourself_',\n",
       "  '_élite_',\n",
       "  '_étape_',\n",
       "  'aaron',\n",
       "  'abace',\n",
       "  'aback',\n",
       "  'abaft',\n",
       "  'abandon',\n",
       "  'abandoned',\n",
       "  'abase',\n",
       "  'abasement',\n",
       "  'abash',\n",
       "  'abashed',\n",
       "  'abate',\n",
       "  'abatement',\n",
       "  'abbaye',\n",
       "  'abbess',\n",
       "  'abbey',\n",
       "  'abbeyland',\n",
       "  'abbot',\n",
       "  'abbots',\n",
       "  'abbé',\n",
       "  'abdicate',\n",
       "  'abdication',\n",
       "  'abduct',\n",
       "  'abdullah',\n",
       "  'abear',\n",
       "  'abed',\n",
       "  'abel',\n",
       "  'abels',\n",
       "  'aberdeen',\n",
       "  'aberdonian',\n",
       "  'aberration',\n",
       "  'abet',\n",
       "  'abeyance',\n",
       "  'abhor',\n",
       "  'abhorrence',\n",
       "  'abhorrent',\n",
       "  'abide',\n",
       "  'abideth',\n",
       "  'abiding',\n",
       "  'abilities',\n",
       "  'ability',\n",
       "  'abject',\n",
       "  'abjectly',\n",
       "  'abjectness',\n",
       "  'abjure',\n",
       "  'ablaze',\n",
       "  'able',\n",
       "  'ablution',\n",
       "  'ablutions',\n",
       "  'ably',\n",
       "  'abnegation',\n",
       "  'abnormal',\n",
       "  'abnormalities',\n",
       "  'abnormally',\n",
       "  'aboard',\n",
       "  'aboardship',\n",
       "  'abolish',\n",
       "  'abolition',\n",
       "  'abominable',\n",
       "  'abominably',\n",
       "  'abominate',\n",
       "  'abomination',\n",
       "  'aboriginal',\n",
       "  'aborigines',\n",
       "  'abortive',\n",
       "  'abound',\n",
       "  'abraham',\n",
       "  'abrahams',\n",
       "  'abreast',\n",
       "  'abreuve',\n",
       "  'abridge',\n",
       "  'abroad',\n",
       "  'abrupt',\n",
       "  'abruptly',\n",
       "  'abruptness',\n",
       "  'abscess',\n",
       "  'absence',\n",
       "  'absences',\n",
       "  'absent',\n",
       "  'absentee',\n",
       "  'absently',\n",
       "  'absentminded',\n",
       "  'absinthe',\n",
       "  'absolute',\n",
       "  'absolutely',\n",
       "  'absolution',\n",
       "  'absolve',\n",
       "  'absorb',\n",
       "  'absorbed',\n",
       "  'absorbing',\n",
       "  'absorption',\n",
       "  'abstain',\n",
       "  'abstinence',\n",
       "  'abstract',\n",
       "  'abstractedly',\n",
       "  'abstraction',\n",
       "  'abstractly',\n",
       "  'abstruse',\n",
       "  'absurd',\n",
       "  'absurdest',\n",
       "  'absurdities',\n",
       "  'absurdity',\n",
       "  'absurdly',\n",
       "  'abudah',\n",
       "  'abundance',\n",
       "  'abundant',\n",
       "  'abundantly',\n",
       "  'abune',\n",
       "  'abuse',\n",
       "  'abusive',\n",
       "  'abut',\n",
       "  'academic',\n",
       "  'academy',\n",
       "  'académicien',\n",
       "  'accede',\n",
       "  'accelerate',\n",
       "  'accent',\n",
       "  'accentuate',\n",
       "  'accept',\n",
       "  'acceptability',\n",
       "  'acceptable',\n",
       "  'acceptance',\n",
       "  'acceptation',\n",
       "  'access',\n",
       "  'accessible',\n",
       "  'accession',\n",
       "  'accessories',\n",
       "  'accessory',\n",
       "  'accident',\n",
       "  'accidental',\n",
       "  'accidentally',\n",
       "  'accidented',\n",
       "  'accidently',\n",
       "  'accidents',\n",
       "  'acclaim',\n",
       "  'acclamation',\n",
       "  'acclamations',\n",
       "  'acclimatization',\n",
       "  'accolade',\n",
       "  'accommodate',\n",
       "  'accommodation',\n",
       "  'accommodations',\n",
       "  'accompagnerez',\n",
       "  'accompaniment',\n",
       "  'accompaniments',\n",
       "  'accompany',\n",
       "  'accompli_',\n",
       "  'accomplice',\n",
       "  'accomplices',\n",
       "  'accomplish',\n",
       "  'accomplishment',\n",
       "  'accomplishments',\n",
       "  'accord',\n",
       "  'accordance',\n",
       "  'accordant',\n",
       "  'accordin',\n",
       "  'according',\n",
       "  'accordingly',\n",
       "  'accost',\n",
       "  'account',\n",
       "  'accountable',\n",
       "  'accountant',\n",
       "  'accountants',\n",
       "  'accounts',\n",
       "  'accoutre',\n",
       "  'accoutrement',\n",
       "  'accredit',\n",
       "  'accrue',\n",
       "  'accumulate',\n",
       "  'accumulation',\n",
       "  'accumulations',\n",
       "  'accumulative',\n",
       "  'accuracy',\n",
       "  ...])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "len(terms), terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T12:03:56.659161Z",
     "start_time": "2020-11-02T12:03:43.608658Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda_model = LatentDirichletAllocation(n_components=5, learning_method='online', random_state=13, max_iter=1)\n",
    "lda_top = lda_model.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T12:03:56.664090Z",
     "start_time": "2020-11-02T12:03:56.660817Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.22398007 0.23889574 0.84701472 ... 0.20055412 0.20050273 0.20000566]\n",
      " [0.20174918 0.20200475 0.26753109 ... 0.20123267 0.20059365 0.20000062]\n",
      " [3.23702477 0.65242872 0.34478596 ... 0.34498558 0.20066732 0.2000005 ]\n",
      " [0.20160278 0.20292731 0.49657982 ... 0.20218075 0.20071678 0.20002565]\n",
      " [0.32059614 0.43018788 0.31937211 ... 0.2003253  0.20000534 0.20000059]]\n"
     ]
    }
   ],
   "source": [
    "print(lda_model.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T12:03:56.669462Z",
     "start_time": "2020-11-02T12:03:56.666564Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 28005)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T12:03:56.695829Z",
     "start_time": "2020-11-02T12:03:56.671210Z"
    }
   },
   "outputs": [],
   "source": [
    "terms = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T12:10:45.953945Z",
     "start_time": "2020-11-02T12:10:45.950244Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_authors(components, feature_names, n=10):\n",
    "    for idx, author in enumerate(components):\n",
    "        print(\"Author %d :\" % (idx+1), [(feature_names[i], author[i].round(2)) for i in author.argsort()[:-n -1:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T12:10:46.541776Z",
     "start_time": "2020-11-02T12:10:46.525519Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author 1 : [('odin', 3219.23), ('know', 1003.03), ('come', 1000.3), ('think', 883.42), ('look', 771.44), ('make', 658.49), ('like', 617.74), ('time', 582.27), ('hand', 571.71), ('good', 561.44)]\n",
      "Author 2 : [('odin', 175.11), ('mean', 139.87), ('address', 118.3), ('ivan', 111.26), ('cross', 105.86), ('paper', 100.44), ('surely', 96.82), ('certain', 92.19), ('angry', 91.26), ('assure', 89.74)]\n",
      "Author 3 : [('exclaim', 115.01), ('dora', 100.46), ('tell', 78.63), ('pray', 66.61), ('knight', 58.42), ('pleasant', 57.08), ('squire', 53.57), ('demand', 53.06), ('hush', 49.24), ('amazement', 48.81)]\n",
      "Author 4 : [('inquire', 159.2), ('horse', 96.9), ('guess', 79.6), ('shoot', 69.23), ('bend', 63.21), ('black', 56.3), ('professor', 54.76), ('pardon', 53.58), ('ride', 49.7), ('bird', 48.85)]\n",
      "Author 5 : [('pretty', 128.54), ('truth', 115.12), ('kill', 96.74), ('shout', 96.63), ('water', 85.37), ('pull', 76.1), ('clerk', 67.57), ('small', 66.22), ('just', 65.83), ('odin', 62.34)]\n"
     ]
    }
   ],
   "source": [
    "get_authors(lda_model.components_, terms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
